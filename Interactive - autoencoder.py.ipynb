{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d9c109-1203-4dac-b811-ce4336199379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=64, out_features=16, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): UnsqueezeLayer()\n",
      "    (2): RepeatLayer()\n",
      "    (3): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(3,), padding=(1,), output_padding=(1,))\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(3,), padding=(1,), output_padding=(1,))\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "    (7): ConvTranspose1d(16, 1, kernel_size=(5,), stride=(3,), padding=(2,), output_padding=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', negative_slope=0.01, temperature=.5,\n",
    "                 dropout=.5, **kwargs):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        encoder_layers.append(torch.nn.Dropout1d(dropout))\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class RepeatLayer(torch.nn.Module):\n",
    "    def __init__(self, repeats, dim):\n",
    "        super(RepeatLayer, self).__init__()\n",
    "        self.repeats = repeats\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat_interleave(self.repeats, dim=self.dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnsqueezeLayer(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(UnsqueezeLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(self.dim)\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5, **kwargs):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        encoder_layers.append(torch.nn.Flatten())\n",
    "        encoder_layers.append(torch.nn.Linear(prev_channels, latent_dim))\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_strides = list(reversed(strides))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        decoder_layers.append(torch.nn.Linear(latent_dim, reversed_hidden_channels[0]))\n",
    "        decoder_layers.append(UnsqueezeLayer(dim=-1))\n",
    "        decoder_layers.append(RepeatLayer(7, dim=-1))\n",
    "        prev_channels = reversed_hidden_channels[0]\n",
    "        for hidden_channel, stride, kernel_size in zip(reversed_hidden_channels[1:], reversed_strides[:-1], reversed_kernel_sizes[:-1]):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, reversed_kernel_sizes[-1], stride=reversed_strides[-1], padding=2, output_padding=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        out = self.decoder(z)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 16\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 24])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=2, stride=1),\n",
    "    torch.nn.Conv1d(in_channels=8, out_channels=16, kernel_size=4, stride=2),\n",
    "    torch.nn.Conv1d(in_channels=16, out_channels=32, kernel_size=6, stride=4)\n",
    ")\n",
    "\n",
    "encoder(torch.randn(64, 1, 200)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 199])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=6, stride=4),\n",
    "    torch.nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=4, stride=2, output_padding=1),\n",
    "    torch.nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=2, stride=1),\n",
    ")\n",
    "\n",
    "decoder(encoder(torch.randn(64, 1, 200))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
