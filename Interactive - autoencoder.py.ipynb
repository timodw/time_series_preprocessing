{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8ba840-4658-43b1-ab36-0f1b140482ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:265\u001b[0m\n\u001b[1;32m    262\u001b[0m kernel_sizes \u001b[39m=\u001b[39m [\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m]\n\u001b[1;32m    263\u001b[0m latent_dim \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m--> 265\u001b[0m autoencoder \u001b[39m=\u001b[39m ConvolutionalCategoricalAutoencoder(\n\u001b[1;32m    266\u001b[0m     \u001b[39m# input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     input_channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    268\u001b[0m     input_length\u001b[39m=\u001b[39;49minput_dim,\n\u001b[1;32m    269\u001b[0m     hidden_channels\u001b[39m=\u001b[39;49mhidden_channels,\n\u001b[1;32m    270\u001b[0m     kernel_sizes\u001b[39m=\u001b[39;49mkernel_sizes,\n\u001b[1;32m    271\u001b[0m     latent_dim\u001b[39m=\u001b[39;49mlatent_dim,\n\u001b[1;32m    272\u001b[0m     activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlrelu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    273\u001b[0m     latent_activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    274\u001b[0m     negative_slope\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[39mprint\u001b[39m(autoencoder)\n",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:194\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n\u001b[1;32m    192\u001b[0m              activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlrelu\u001b[39m\u001b[39m'\u001b[39m, latent_activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlrelu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    193\u001b[0m              negative_slope\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m     \u001b[39msuper\u001b[39;49m(CategoricalAutoencoder, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_activation_function(activation, negative_slope)\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent_activation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_activation_function(latent_activation, negative_slope)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, kernel_size in zip(hidden_channels, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        # input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b10a06-57fb-4a99-bbc9-bdfab1c7b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (7): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, kernel_size in zip(hidden_channels, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d663e8-41ae-451c-a067-a8009c2e684a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mencode(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "p = self.encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0d07ce-e74b-49ec-86e8-3303393f5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (7): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, kernel_size in zip(hidden_channels, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        print(p.size())\n",
    "        return p\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1073, -0.1115, -0.1075,  ..., -0.1236, -0.1223, -0.1728],\n",
       "         [-0.0858, -0.0914, -0.0766,  ..., -0.0272, -0.0955, -0.0508],\n",
       "         [-0.0437, -0.0297, -0.0391,  ..., -0.0260, -0.0033, -0.0131],\n",
       "         ...,\n",
       "         [-0.0491, -0.0589, -0.0766,  ..., -0.0683, -0.0319, -0.0650],\n",
       "         [-0.0364, -0.0838, -0.0981,  ..., -0.0477, -0.0723, -0.1099],\n",
       "         [-0.0244, -0.0408, -0.0766,  ..., -0.0615, -0.0137, -0.0135]],\n",
       "\n",
       "        [[-0.1078, -0.1388, -0.1500,  ..., -0.0940, -0.0858, -0.1200],\n",
       "         [-0.0717, -0.1100, -0.0860,  ..., -0.0814, -0.0907, -0.0799],\n",
       "         [-0.0360, -0.0429, -0.0480,  ..., -0.0768, -0.0655, -0.0292],\n",
       "         ...,\n",
       "         [-0.0861, -0.0401, -0.0617,  ..., -0.0444, -0.0716, -0.0124],\n",
       "         [-0.0974, -0.0964, -0.1060,  ..., -0.0569, -0.0995, -0.0908],\n",
       "         [-0.0645, -0.0706, -0.0610,  ..., -0.1090, -0.0796, -0.0193]],\n",
       "\n",
       "        [[-0.1082, -0.0666, -0.0803,  ..., -0.0886, -0.0841, -0.1090],\n",
       "         [-0.1283, -0.1053, -0.0668,  ..., -0.0589, -0.0886, -0.1132],\n",
       "         [-0.0410, -0.0373, -0.0286,  ..., -0.0579, -0.0424, -0.0251],\n",
       "         ...,\n",
       "         [-0.0536, -0.0115, -0.0354,  ..., -0.0034, -0.0294, -0.0745],\n",
       "         [-0.0528, -0.0331, -0.0706,  ..., -0.0520, -0.0213, -0.0722],\n",
       "         [-0.0359, -0.0371,  0.0081,  ..., -0.0464, -0.0614, -0.0635]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1448, -0.1212, -0.1343,  ..., -0.0859, -0.0733, -0.1129],\n",
       "         [-0.1029, -0.0812, -0.0651,  ..., -0.0942, -0.0715, -0.0456],\n",
       "         [-0.0393, -0.0226, -0.0319,  ..., -0.0415, -0.0577, -0.0220],\n",
       "         ...,\n",
       "         [-0.0475, -0.0751, -0.0607,  ..., -0.0814, -0.0693, -0.0520],\n",
       "         [-0.1101, -0.0818, -0.1099,  ..., -0.0876, -0.0477, -0.0988],\n",
       "         [-0.0321, -0.0889, -0.0507,  ..., -0.0840, -0.0796, -0.0446]],\n",
       "\n",
       "        [[-0.1031, -0.1363, -0.1038,  ..., -0.1074, -0.1154, -0.1307],\n",
       "         [-0.0910, -0.0820, -0.0833,  ..., -0.1233, -0.0470, -0.0483],\n",
       "         [-0.0513, -0.0700, -0.0490,  ...,  0.0048, -0.0187, -0.0187],\n",
       "         ...,\n",
       "         [-0.0127, -0.0840, -0.0895,  ..., -0.0181, -0.0471, -0.0203],\n",
       "         [-0.0798, -0.0640, -0.0800,  ..., -0.0987, -0.0974, -0.0591],\n",
       "         [-0.0627, -0.0899, -0.0955,  ..., -0.0402, -0.0728, -0.0331]],\n",
       "\n",
       "        [[-0.0733, -0.1146, -0.1376,  ..., -0.1123, -0.1175, -0.1340],\n",
       "         [-0.0054, -0.1042, -0.0993,  ..., -0.1095, -0.1041, -0.0368],\n",
       "         [-0.0161, -0.0463, -0.0121,  ...,  0.0077, -0.0535, -0.0437],\n",
       "         ...,\n",
       "         [-0.0368, -0.1171, -0.0354,  ..., -0.0470, -0.0848, -0.0293],\n",
       "         [-0.0648, -0.0603, -0.0947,  ..., -0.0696, -0.1154, -0.0892],\n",
       "         [-0.0703, -0.0548, -0.0726,  ..., -0.0447, -0.1148, -0.0521]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72349a97-eee5-4e07-bbbe-cdba14e2cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (7): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, kernel_size in zip(hidden_channels, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 99])\n",
      "torch.Size([64, 16, 99])\n",
      "torch.Size([64, 32, 49])\n",
      "torch.Size([64, 32, 49])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoencoder(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m64\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:258\u001b[0m\n\u001b[1;32m    256\u001b[0m     x \u001b[39m=\u001b[39m l(x)\n\u001b[1;32m    257\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39msize())\n\u001b[0;32m--> 258\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(p, temperature\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature)\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z), p\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 99])\n",
      "torch.Size([64, 16, 99])\n",
      "torch.Size([64, 32, 49])\n",
      "torch.Size([64, 32, 49])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoencoder(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m64\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 258\u001b[0m, in \u001b[0;36mConvolutionalCategoricalAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     x \u001b[39m=\u001b[39m l(x)\n\u001b[1;32m    257\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39msize())\n\u001b[0;32m--> 258\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(p, temperature\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature)\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z), p\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4c08db-def0-4fad-a965-82c1c153a574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (7): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length, hidden_channels, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, kernel_size in zip(hidden_channels, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 99])\n",
      "torch.Size([64, 16, 99])\n",
      "torch.Size([64, 32, 49])\n",
      "torch.Size([64, 32, 49])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n",
      "torch.Size([64, 64, 24])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faba2826-8dbf-4275-a279-19607c30e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 16, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv1d(32, 32, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Conv1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,))\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (13): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(32, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): ConvTranspose1d(16, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 16, 32, 32, 64, 64]\n",
    "    strides = [1, 2, 1, 2, 1, 2]\n",
    "    kernel_sizes = [5, 5, 5, 5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 198])\n",
      "torch.Size([64, 16, 198])\n",
      "torch.Size([64, 16, 98])\n",
      "torch.Size([64, 16, 98])\n",
      "torch.Size([64, 32, 96])\n",
      "torch.Size([64, 32, 96])\n",
      "torch.Size([64, 32, 47])\n",
      "torch.Size([64, 32, 47])\n",
      "torch.Size([64, 64, 45])\n",
      "torch.Size([64, 64, 45])\n",
      "torch.Size([64, 64, 22])\n",
      "torch.Size([64, 64, 22])\n",
      "torch.Size([64, 64, 22])\n",
      "torch.Size([64, 64, 22])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2404313b-8e5f-4e0f-8b98-106edebbe913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (7): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.Conv1d(prev_channels, latent_dim, kernel_size=1))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4706357-81f4-4d2d-9652-6ce2e4120e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771b355b-1410-44f1-8e1e-c8c2ca223221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    (7): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (8): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        encoder_layers.append(torch.nn.Linear(prev_channels, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4096x1 and 64x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoencoder(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m64\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:258\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    256\u001b[0m     \u001b[39m# p = self.encode(x)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder:\n\u001b[0;32m--> 258\u001b[0m         x \u001b[39m=\u001b[39m l(x)\n\u001b[1;32m    259\u001b[0m         \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39msize())\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4096x1 and 64x64)"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c16845b5-b310-4494-8b60-9e588d59d286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        encoder_layers.append(torch.nn.Flatten())\n",
    "        encoder_layers.append(torch.nn.Linear(prev_channels, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 64\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 1])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46747933-cf9d-4f7e-9071-27675471a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (9): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(16, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        encoder_layers.append(torch.nn.Flatten())\n",
    "        encoder_layers.append(torch.nn.Linear(prev_channels, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p = self.encode(x)\n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            print(x.size())\n",
    "        return\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 16\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 16, 66])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 32, 22])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 7])\n",
      "torch.Size([64, 64, 1])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 16])\n",
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e71e07-cc20-4db7-a762-9c9dd479cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (9): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(16, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        encoder_layers.append(torch.nn.Flatten())\n",
    "        encoder_layers.append(torch.nn.Linear(prev_channels, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        print(p.size())\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 16\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoencoder(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m64\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "455cde64-0204-44c5-aae0-634e4c12d291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalCategoricalAutoencoder(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (latent_activation): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(32, 64, kernel_size=(5,), stride=(3,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (9): Identity()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(16, 64, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): ConvTranspose1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=1E-3):\n",
    "    BCE = torch.nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + kl_weight * KLD\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, activation='lrelu', latent_activation='lrelu', negative_slope=0.01):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = torch.nn.Linear(prev_dim, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(prev_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def gumbel_elbo_loss(X_pred, X_true, p, kl_weight=1E-3):\n",
    "    rec_loss = torch.nn.functional.mse_loss(X_pred, X_true)\n",
    "    logits = torch.nn.functional.log_softmax(p, dim=-1)\n",
    "    kl = torch.nn.functional.kl_div(logits, torch.ones_like(logits) / p.size()[-1])\n",
    "    return rec_loss + kl_weight * kl\n",
    "\n",
    "class CategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=.5):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(torch.nn.Linear(prev_dim, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(torch.nn.Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(torch.nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "class ConvolutionalCategoricalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels, input_length,\n",
    "                 hidden_channels, strides, kernel_sizes, latent_dim,\n",
    "                 activation='lrelu', latent_activation='lrelu',\n",
    "                 negative_slope=0.01, temperature=0.5):\n",
    "        super(ConvolutionalCategoricalAutoencoder, self).__init__()\n",
    "\n",
    "        self.activation = self._get_activation_function(activation, negative_slope)\n",
    "        self.latent_activation = self._get_activation_function(latent_activation, negative_slope)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_channels = input_channels\n",
    "        for hidden_channel, stride, kernel_size in zip(hidden_channels, strides, kernel_sizes):\n",
    "            encoder_layers.append(torch.nn.Conv1d(prev_channels, hidden_channel, kernel_size, stride=stride, padding=1))\n",
    "            encoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        encoder_layers.append(torch.nn.AdaptiveMaxPool1d(1))\n",
    "        encoder_layers.append(torch.nn.Flatten())\n",
    "        encoder_layers.append(torch.nn.Linear(prev_channels, latent_dim))\n",
    "        encoder_layers.append(self.latent_activation)\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_channels = latent_dim\n",
    "        reversed_hidden_channels = list(reversed(hidden_channels))\n",
    "        reversed_kernel_sizes = list(reversed(kernel_sizes))\n",
    "        for hidden_channel, kernel_size in zip(reversed_hidden_channels, reversed_kernel_sizes):\n",
    "            decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, hidden_channel, kernel_size, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(self.activation)\n",
    "            prev_channels = hidden_channel\n",
    "        decoder_layers.append(torch.nn.ConvTranspose1d(prev_channels, input_channels, kernel_size=1))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "\n",
    "    def _get_activation_function(self, activation, negative_slope):\n",
    "        if activation == 'lrelu':\n",
    "            return torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        elif activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif activation == 'linear':\n",
    "            return torch.nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        p = self.encoder(x)\n",
    "        return p\n",
    "\n",
    "    def reparameterize(self, p, temperature=0.5, epsilon=1E-7):\n",
    "        g = torch.rand_like(p)\n",
    "        g = -torch.log(-torch.log(g + epsilon) + epsilon)\n",
    "\n",
    "        z = torch.nn.functional.softmax((p + g) / temperature, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.encode(x)\n",
    "        print(p.size())\n",
    "        z = self.reparameterize(p, temperature=self.temperature)\n",
    "        return self.decode(z), p\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dim = 200\n",
    "    hidden_channels = [16, 32, 64]\n",
    "    strides = [3, 3, 3]\n",
    "    kernel_sizes = [5, 5, 5]\n",
    "    latent_dim = 16\n",
    "\n",
    "    autoencoder = ConvolutionalCategoricalAutoencoder(\n",
    "        input_channels=1,\n",
    "        input_length=input_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        strides=strides,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        latent_dim=latent_dim,\n",
    "        activation='lrelu',\n",
    "        latent_activation='linear',\n",
    "        negative_slope=0.01\n",
    "    )\n",
    "\n",
    "    print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [16, 64, 5], expected input[1, 64, 16] to have 16 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoencoder(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m64\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m))[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:260\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mprint\u001b[39m(p\u001b[39m.\u001b[39msize())\n\u001b[1;32m    259\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(p, temperature\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature)\n\u001b[0;32m--> 260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(z), p\n",
      "File \u001b[1;32m/home/timodw/IDLab/time_series_preprocessing/autoencoder.py:254\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, z):\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(z)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/nn/modules/conv.py:797\u001b[0m, in \u001b[0;36mConvTranspose1d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    794\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    795\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose1d(\n\u001b[1;32m    798\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    799\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [16, 64, 5], expected input[1, 64, 16] to have 16 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "autoencoder(torch.randn(64, 1, 200))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
